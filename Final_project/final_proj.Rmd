---
title: "MA415 Final Project- Analysis of Tweets About President Trump"
author: "Sibo Zhu"
date: "2017/12/15"
output: pdf_document
---

##Introduction

I believe celebrities' work can be reflected by how people talk about them. As President Trump has inaugurated for already an whole year, how people discuss about him within their social media could express their current attitude towards President Trump, and the work he has done.\par 
By utilizing the Twitter api, with all the tweets contains hashtag about Trump and the geo location info, researches about regions that interested in President Trump and related analysis would also be useful.\par



```{r setup, include = TRUE}
### prepare packages
library(ROAuth)
library(streamR)
library(twitteR)
library(tm)
library(SnowballC)
library(ggplot2)
library(ggvis)
library(ggmap)
library(RgoogleMaps)
library(devtools)
library(maps)
library(mapdata)
library(grid) 
library(dplyr)
library(wordcloud)
library(plyr)
library(shiny)
library(stringr)
library(dplyr)
library(wordcloud)
library(gridExtra)
library(topicmodels)
```


```{r}
### setting up connections for Twitter 
#requestURL <- "https://api.twitter.com/oauth/request_token"
#accessURL <- "https://api.twitter.com/oauth/access_token"
#authURL <- "https://api.twitter.com/oauth/authorize"
#api_key             <- "dSzLRlqni1W2f9CJagS19RP6b"
#api_secret          <- "pLX7gIERogxsgn7AT2eEAQlhmxGKwEx9de5pjeOtjkQDDbVVCD"
#access_token        <- "3134023545-OZ9TIdXFbEFW66HmsCUo0EWGIANJ4SgS1OgdDO7"
#access_token_secret <- "kAzcVK0EMOhCFAs13Mau2Y8q2fCYHjvA5XXl7Rd0SCO1o"


###Above codes are my personal Twitter App Keys, I shouldn't have included them in this project, 
###but I just did so for your convenience, so please don't use them to do bad things

### Setting up streamR

#my_oauth <- OAuthFactory$new(consumerKey = api_key, consumerSecret = api_secret, 
#requestURL = requestURL, accessURL = accessURL, authURL = authURL)
#my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
#save(my_oauth, file = "my_oauth.Rdata")      #Save my Oauth info for future convenience



```

```{r}
#load("my_oauth.Rdata")     #Load my Oauth file save above to avoid authentication everytime running the code

#  filterStream( file="D_Tweet.json", track="realDonaldTrump",  #naming the downloaded file as "D_Tweet.json", and hashtag with "realDonaldTrump"
#      locations=c(-74,40,-73,41), timeout=7200, oauth=my_oauth )     #restrict tweets within United States area, capturing tweets within 2 hours

###Since it would take 2 hours to complete the capturing, it's certainly meaningless to let this chunk run for each run; 
###If you'd like to run the code yourself, you are welcome to uncomment the above lines and go for it;
###Also, due to Github's regulation of file size restriction of 100MB maximum, I cannot upload my "D_Tweet.json", but all the
### data in that json file will be cleaned and write into "cleaned_tweets_df.csv" later
```

```{r}

#tweets_raw.df <- parseTweets("DT_Tweet.json",verbose = FALSE)


#keep <- c("text","lang","listed_count","geo_enabled","statuses_count","followers_count",
#          "favourites_count","friends_count","time_zone","country_code","full_name",
#          "place_lat","place_lon")

#tweets.df <- tweets_raw.df[,keep];


#write.csv(tweets.df,"tweets_df.csv") 

#save(tweets.df, file = "tweets.df.Rdata")      #save data for future convenience
```

```{r}

load("tweets.df.Rdata")      #load data

tweets.df <- tweets.df[tweets.df$lang=="en",]     #filtering for only English tweets
tweets.df <- tweets.df[tweets.df$country_code=="US",]     #filtering for US region tweets
tweets.df <- tweets.df[tweets.df$geo_enabled==TRUE,]     #filtering for goelocations enabled tweets

write.csv(tweets.df,"cleaned_tweets_df.csv")     #writing cleaned data into CSV 
```

```{r,warning=FALSE,message=FALSE}
#filtering out three typical states that worth analyzing
ca <- data.frame(filter(tweets.df, grepl(', CA', full_name)))
nj <- data.frame(filter(tweets.df, grepl(', NJ', full_name)))
ny <- data.frame(filter(tweets.df, grepl(', NY', full_name)))

###Plotting
NJplot <- qmplot(place_lon, place_lat, data = nj, colour = I('purple'), size = I(2), alpha=I(0.4),
       mapcolor = "bw", main="New Jersey")     
NYplot <- qmplot(place_lon, place_lat, data = ny, colour = I('darkred'), size = I(2), alpha=I(0.4),
       mapcolor = "bw", main="New York")
CAplot <- qmplot(place_lon, place_lat, data = ca, colour = I('blue'), size = I(2), alpha=I(0.4),
       mapcolor = "bw",main="California")

###Plotting data within whole USA region
USMap <- get_map (location = c(lon = -95.71289, lat = 37.09024), zoom=4, scale=2, maptype="roadmap", source="google",crop=TRUE)
USAplot <- ggmap(USMap) +
  geom_point(data = tweets.df, aes(x=tweets.df$place_lon,y=tweets.df$place_lat),alpha=0.4,color="red") +
  ggtitle("US Map for the Total Dataset")
```


```{r}
NYplot
NJplot
CAplot
USAplot
#Analysis about states and USA plots goes here
```


```{r}
###Adding a new variable named "state" for future usage
ny[,"state"] <- rep("NY",nrow(ny));
nj[,"state"] <- rep("NJ",nrow(nj));
ca[,"state"] <- rep("CA",nrow(ca));

###getting city's names
ny$full_name <- as.character(ny$full_name);
nj$full_name <- as.character(nj$full_name);
ca$full_name <- as.character(ca$full_name);
tweets.df$full_name <- as.character(tweets.df$full_name);

### helping function that used for capturing cities
city_get <- function(x){
  city_name <- c()
  name <- strsplit(x$full_name,",")
  for(i in 1:nrow(x)){
    city_name <- c(city_name,name[[i]][1])
  }
  return(city_name)
}

ny[,"city"] <- factor(city_get(ny));
nj[,"city"] <- factor(city_get(nj));
ca[,"city"] <- factor(city_get(ca));
tweets.df[,"city"] <- factor(city_get(tweets.df))
total_us <- filter(tweets.df,lang=="en")
```

```{r}
###Counting tweets for every city in each states
count <- summary(ny$city)[1:15];ny_city_count <- as.data.frame(count)
count <- summary(nj$city)[1:15];nj_city_count <- as.data.frame(count)
count <- summary(ca$city)[1:15];ca_city_count <- as.data.frame(count)
###Do the same thing towards whole United States region
count <- summary(tweets.df$city)[1:15];us_city_count <- as.data.frame(count)


NYCplot <- ggplot(ny_city_count,aes(reorder(rownames(ny_city_count),count),count))+
  geom_bar(stat = "identity")+coord_flip()+xlab("City")+ggtitle("NY")
#nycityplot
NJCyplot <- ggplot(nj_city_count,aes(reorder(rownames(nj_city_count),count),count))+
  geom_bar(stat = "identity")+coord_flip()+xlab("City")+ggtitle("NJ")
#njcityplot
CACplot <- ggplot(ca_city_count,aes(reorder(rownames(ca_city_count),count),count))+
  geom_bar(stat = "identity")+coord_flip()+xlab("City")+ggtitle("CA")
#cacityplot
#grid.arrange(nycityplot, njcityplot, cacityplot, ncol=3)

USCplot <- ggplot(us_city_count,aes(reorder(rownames(us_city_count),count),count))+
  geom_bar(stat = "identity")+coord_flip()+xlab("City")+ggtitle("United States")

#uscityplot

```


```{r}
#Analysis about cities and USA tweets count goes here
```


```{r}
pairs(~listed_count+statuses_count+followers_count+favourites_count+friends_count, data=total_us)
cor(total_us[,c(3,5,6,7,8)])
```

```{r}
#pairs and correlations goes here
```


```{r}

model_fl <- lm(followers_count~listed_count,data=total_us)
summary(model_fl)
regplot <- ggplot(total_us,aes(x=listed_count,y=followers_count))+geom_point()+geom_smooth(method = "lm")
regplot
#analysis of regression goes here
par(mfrow=c(1,2))
plot(model_fl,1)
plot(model_fl,2)
```


```{r}
#wordcloud

total_us_word <- total_us
total_us_word <- total_us_word[total_us_word$lang=="en",]
total_us_word <- total_us_word[total_us_word$geo_enabled==TRUE,]
total_us_word <- total_us_word[total_us_word$place_lat >25 & 
                    total_us_word$place_lat <50 & total_us_word$place_lon > -125 & 
                    total_us_word$place_lon< -66,]
total_us_word$geo_enabled <- NULL

# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(total_us_word$text))
# remove anything other than English letters or space(!!!)
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove stopwords
myStopwords <- c(stopwords('english'),"use", "see", "used", "via", "amp","im")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)

# Build Term Document Matrix
tdm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
term.freq <- rowSums(as.matrix(tdm))
term.freq2 <- subset(term.freq, term.freq >= 5)
df <- data.frame(term = names(term.freq2), freq = term.freq2)
m <- as.matrix(tdm)

```

```{r}

# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)

# plot word cloud
wordcloud(words = names(word.freq), 
          freq = word.freq, 
          max.words = 50,
          min.freq = 3, 
          scale = c(4.5, 1),
          colors = brewer.pal(8, "Dark2"),
          random.color = T,
          random.order = F)


#analysis of wordcloud goes here
```

```{r}
###Also, on the other hand, let's see President Trump his wordcloud analysis

###Here I used a new Twitter Application Api keys for different purpose, I also shouldn't have included this but still I did so.

#api_key             <- "RE3jcprdcB3YwIXwcCG2rHPmj"
#api_secret          <- "rPeMG3nqOwDVKtXOuIsd4czCZjJd1eBh9BHqPWuUf8xBDaIY4j"
#access_token        <- "3134023545-cJbCbhXSeklrnVtEKU2yvv3bTI7APn93As93WiS"
#access_token_secret <- "glaUQunnz9e2QQT2gpKNgJHWtqKGRBK12JP9HtN44rCdT"
#setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)

###I comment this chunk out for avoid unneccessary time when kniting the whole project, but you are welcome to 
###uncomment them for running the code throughly. However, since I will store Trump's tweets later and load them in the next chunk,
###There won't be any problem you just leave this chunk commented out.

#user_id <- "@realDonaldTrump"
#trump_tweets_raw <- userTimeline(user_id, n = 3200)

#save(trump_tweets_raw, file = "trump_tweets_raw.Rdata")
```

```{r}
load("trump_tweets_raw.Rdata")

# remove retweets
trump_tweets_df <- twListToDF(strip_retweets(trump_tweets_raw, strip_manual = TRUE, strip_mt = TRUE))

# normalize data
trump_tweets_df$text <- iconv(trump_tweets_df$text, 'latin1', 'ASCII', 'byte')

Trump_Corpus <- Corpus(VectorSource(trump_tweets_df$text))      # build a corpus, and specify the source to be character vectors

removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)     # remove anything other than English letters or space(!!!)
Trump_Corpus <- tm_map(Trump_Corpus, content_transformer(removeNumPunct))

Trump_Corpus <- tm_map(Trump_Corpus, stripWhitespace)     # remove extra whitespace

Trump_Corpus <- tm_map(Trump_Corpus, content_transformer(tolower))      # convert to lower case

myStopwords <- c(stopwords('english'),"use", "see", "used", "via", "amp","im")      # remove stopwords
Trump_Corpus <- tm_map(Trump_Corpus, removeWords, myStopwords)

rm_url   <- function(x) gsub("http[^[:space:]]*", "", x)      # remove urls
Trump_Corpus <- tm_map(Trump_Corpus, content_transformer(rm_url))

Trump_Corpus <- tm_map(Trump_Corpus, removePunctuation)     # remove punctuation



# build term document matrix
Trump_tdm <- TermDocumentMatrix(Trump_Corpus, control = list(wordLengths = c(1, Inf)))
Trump_term.freq <- rowSums(as.matrix(Trump_tdm))
Trump_term.freq <- subset(Trump_term.freq, Trump_term.freq >= 5)
df <- data.frame(term = names(Trump_term.freq), freq = Trump_term.freq)
Trump_m  <- as.matrix(Trump_tdm)

# calculate the frequency of words and sort it by frequency
Trump_word_freq <- sort(rowSums(Trump_m), decreasing = T)

wordcloud(words = names(Trump_word_freq),
          freq  = Trump_word_freq,
          scale = c(4.5, 1),
          max.words = 50,
          min.freq = 3,
          colors = brewer.pal(8, "Dark2"),
          random.color = T,
          random.order = F)
```


```{r}
# remove sparse terms
tdm2 <- removeSparseTerms(tdm,sparse = 0.965)
# showing the words that are left gor the analysis
print(dimnames(tdm2)$Terms)
m2 <- as.matrix(tdm2)
m3 <- t(m2)   # transpose the matrix to cluster documents
set.seed(122)   # set a fixed random seed
k <- 6   # number of clusters
kmeansResult <- kmeans(m3,k)
round(kmeansResult$centers, digits = 3)   # cluster centers
for(i in 1:k){
  cat(paste("cluster ", i, ": ", sep = ""))
  s <- sort(kmeansResult$centers[i,],decreasing = T)
  cat(names(s)[1:5],"\n")
  #print the tweet of every cluster
}
```

```{r}
# cluster terms
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method = "complete")
# show cluster dendrogram 
p <- plot(fit, xlab="")
p <- rect.hclust(fit, k=6)
```